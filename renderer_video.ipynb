{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mvsnerf2/lib/python3.8/site-packages/matplotlib_inline/config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n",
      "/root/miniconda3/envs/mvsnerf2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: DeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import sys,os,imageio\n",
    "root = '/host/home/ubuntu/mvsnerf'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# # models\n",
    "# from models import *\n",
    "# from renderer import *\n",
    "# from data.ray_utils import get_rays\n",
    "# from scipy.spatial.transform import Rotation as R\n",
    "from render_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering video from finetuned ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/host/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerf_root_dir = data_dir/'NeRF_Data'\n",
    "zeiss_root_dir = data_dir/'ZEISS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "scene = 'HeadScan'\n",
    "i_scene = 0\n",
    "cmd = f'''  --datadir {zeiss_root_dir}/{scene}  \\\n",
    "                --dataset_name llff --imgScale_test {1.0}  --netwidth 128 --net_type v0 \\\n",
    "                --use_viewdirs \\\n",
    "                --N_samples 128 \\\n",
    "                --chunk 5120\n",
    "                '''\n",
    "is_finetuned = False # set False if rendering without finetuning\n",
    "if is_finetuned:\n",
    "    cmd += f'--ckpt ./runs_fine_tuning/{scene}-ft/ckpts/latest.tar --use_disp '\n",
    "    # cmd += '--use_color_volume ' # add only if model was finetuned with this option\n",
    "else:\n",
    "    cmd += '--ckpt ./ckpts/mvsnerf-v0.tar'\n",
    "    \n",
    "args = config_parser(cmd.split())\n",
    "# options not included in original option set\n",
    "args.feat_dim = 8+3*4 \n",
    "\n",
    "save_dir = f'results/video2'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "9 9 /host/data/ZEISS/HeadScan\n",
      "===> training index: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "## Create models\n",
    "if i_scene==0 or is_finetuned:\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "    filter_keys(render_kwargs_train)\n",
    "\n",
    "    MVSNet = render_kwargs_train['network_mvs']\n",
    "    render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "datadir = args.datadir\n",
    "datatype = 'train'\n",
    "pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "\n",
    "\n",
    "dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "val_idx = dataset.img_idx\n",
    "\n",
    "MVSNet.train()\n",
    "MVSNet = MVSNet.cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> ref idx: [0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:43<00:00, 38.20s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    c2ws_all = dataset.poses\n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "    if is_finetuned:   # large baseline\n",
    "        volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "        volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "    else:            \n",
    "        # neighboring views with position distance\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad, lindisp=args.use_disp)\n",
    "\n",
    "    pad *= args.imgScale_test\n",
    "    w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    # pair_idx = [i for i in range(9)]\n",
    "    # pdb.set_trace()\n",
    "    c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60)# you can enlarge the rads_scale if you want to render larger baseline        \n",
    "    c2ws_render = c2ws_all # experimental\n",
    "    c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "    \n",
    "    imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "    try:\n",
    "        tqdm._instances.clear() \n",
    "    except Exception:     \n",
    "        pass\n",
    "\n",
    "    frames = []\n",
    "    img_directions = dataset.directions.to(device)\n",
    "    for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "        rays = torch.cat([rays_o, rays_d,\n",
    "                    near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                    near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                1).to(device)  # (H*W, 3)\n",
    "        \n",
    "        \n",
    "        N_rays_all = rays.shape[0]\n",
    "        rgb_rays, depth_rays_preds = [],[]\n",
    "        for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "            xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                N_samples=args.N_samples, lindisp=args.use_disp)\n",
    "\n",
    "            # Converting world coordinate to ndc coordinate\n",
    "            H, W = imgs_source.shape[-2:]\n",
    "            inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "            w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "            xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                            near=near_far_source[0], far=near_far_source[1], pad=pad, lindisp=args.use_disp)\n",
    "\n",
    "\n",
    "            # rendering\n",
    "            rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                    xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                    volume_feature,imgs_source, **render_kwargs_train)\n",
    "\n",
    "            rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "            rgb_rays.append(rgb)\n",
    "            depth_rays_preds.append(depth_pred)\n",
    "\n",
    "        \n",
    "        depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "        depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "        \n",
    "        rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "        H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "        rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "        \n",
    "\n",
    "        frames.append(img_vis.astype('uint8'))\n",
    "imageio.mimwrite(f'{save_dir}/ft_{scene}_spiral{\"\" if is_finetuned else \"_zeroshot\"}.mp4', np.stack(frames), fps=10, quality=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import plotly.graph_objs as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=c2ws_render[:,0,3],\n",
    "    y=c2ws_render[:,1,3],\n",
    "    z=c2ws_render[:,2,3],\n",
    "    mode='markers',\n",
    "    marker=dict(size=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=c2ws_all[:,0,3],\n",
    "    y=c2ws_all[:,1,3],\n",
    "    z=c2ws_all[:,2,3],\n",
    "    mode='markers',\n",
    "    marker=dict(size=2)\n",
    "))\n",
    "fig.update_layout(scene_dragmode='orbit')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "9 9 /host/data/ZEISS/HeadScan\n",
      "===> training index: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "====> ref idx: [0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:45<00:00, 38.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for i_scene, scene in enumerate(['HeadScan']):#'horns','flower','orchids', 'room','leaves','fern','trex','fortress'\n",
    "    # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "    cmd = f'--datadir {zeiss_root_dir}/{scene}  \\\n",
    "     --dataset_name llff --imgScale_test {1.0}  --netwidth 128 --net_type v0 '\n",
    "\n",
    "    is_finetuned = False # set False if rendering without finetuning\n",
    "    if is_finetuned:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/{scene}-ft/ckpts/latest.tar --use_disp '\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts/mvsnerf-v0.tar'\n",
    "        \n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "    # args.use_color_volume = False if not is_finetuned else args.use_color_volume\n",
    "\n",
    "    # create models\n",
    "    if i_scene==0 or is_finetuned:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        c2ws_all = dataset.poses\n",
    "\n",
    "        if is_finetuned:   \n",
    "            # large baseline\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "            \n",
    "            pad *= args.imgScale_test\n",
    "            w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "            pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "            # pdb.set_trace()\n",
    "            c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60)# you can enlarge the rads_scale if you want to render larger baseline\n",
    "        else:            \n",
    "            # neighboring views with position distance\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad, lindisp=args.use_disp)\n",
    "            \n",
    "            pad *= args.imgScale_test\n",
    "            w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "            pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "            # pdb.set_trace()\n",
    "            c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60)# you can enlarge the rads_scale if you want to render larger baseline\n",
    "        c2ws_render = c2ws_all\n",
    "        c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "            \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples, lindisp=args.use_disp)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad, lindisp=args.use_disp)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "                \n",
    "    imageio.mimwrite(f'{save_dir}/ft_{scene}_spiral{\"\" if is_finetuned else \"_zeroshot\"}.mp4', np.stack(frames), fps=10, quality=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blender Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./runs_fine_tuning/hotdog-ft/ckpts//latest.tar']\n",
      "Reloading from ./runs_fine_tuning/hotdog-ft/ckpts//latest.tar\n",
      "===> valing index: [26 60 13 47]\n",
      "====> ref idx: [48 61  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:02:32<00:00, 62.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./runs_fine_tuning/lego-ft/ckpts//latest.tar']\n",
      "Reloading from ./runs_fine_tuning/lego-ft/ckpts//latest.tar\n",
      "===> valing index: [63, 70, 18, 28]\n",
      "====> ref idx: [6, 43, 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:02:27<00:00, 62.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./runs_fine_tuning/mic-ft/ckpts//latest.tar']\n",
      "Reloading from ./runs_fine_tuning/mic-ft/ckpts//latest.tar\n",
      "===> valing index: [20 49 55 72]\n",
      "====> ref idx: [61 80 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 48/60 [49:43<11:13, 56.09s/it]"
     ]
    }
   ],
   "source": [
    "for i_scene, scene in enumerate(['hotdog','lego', 'mic', 'ship', 'drums', 'chair']):#'ship','drums','ficus','materials',\n",
    "\n",
    "    cmd = f'--datadir {nerf_root_dir}/nerf_synthetic/{scene}\\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0} '\n",
    "\n",
    "    is_finetuned = True # set True if rendering with finetuning\n",
    "    if is_finetuned:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/{scene}-ft/ckpts//latest.tar'\n",
    "        pad = 0 #the padding value should be same as your finetuning ckpt\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "        pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "        \n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4\n",
    "#     args.use_color_volume = False if not is_finetuned else args.use_color_volume\n",
    "\n",
    "    # create models\n",
    "    if i_scene==0 or is_finetuned:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    args.chunk = 5120\n",
    "    frames = 60\n",
    "\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = False\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        if is_finetuned:   \n",
    "            # large baselien\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "            c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=frames)\n",
    "        else:            \n",
    "            # neighboring views with angle distance\n",
    "            c2ws_all = dataset.load_poses_all()\n",
    "            random_selete = torch.randint(0,len(c2ws_all),(1,))     #!!!!!!!!!! you may change this line if rendering a specify view \n",
    "            dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n",
    "            pair_idx = np.argsort(dis)[::-1][torch.randperm(5)[:3]]\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)#pair_idx=pair_idx, \n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            \n",
    "            #####\n",
    "            c2ws_render = gen_render_path(c2ws_all[pair_idx], N_views=frames)\n",
    "            c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "#             rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#             depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "#             break\n",
    "    imageio.mimwrite(f'{save_dir}/ft_{scene}_spiral.mp4', np.stack(frames), fps=10, quality=10)\n",
    "# plt.imshow(rgb_rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_scene, scene in enumerate([1]):# any scene index, like 1,2,3...,,8,21,103,114\n",
    "\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene} \\\n",
    "     --dataset_name dtu_ft --imgScale_test {1.0} ' #--use_color_volume\n",
    "    \n",
    "    is_finetuned = True # set False if rendering without finetuning\n",
    "    if is_finetuned:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/dtu_scan{scene}_2h/ckpts//latest.tar'\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts/mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "    args.use_color_volume = False if not is_finetuned else args.use_color_volume\n",
    "\n",
    "\n",
    "    # create models\n",
    "    if i_scene==0 or is_finetuned:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "    args.chunk = 5120\n",
    "    frames = 60\n",
    "\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = False\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if is_finetuned:   \n",
    "            # large baselien\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "        else:            \n",
    "            # neighboring views with angle distance\n",
    "            c2ws_all = dataset.load_poses_all()\n",
    "            random_selete = torch.randint(0,len(c2ws_all),(1,)) #!!!!!!!!!! you may change this line if rendering a specify view \n",
    "            dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n",
    "            pair_idx = np.argsort(dis)[::-1][:3]#[25, 21, 33]#[14,15,24]#\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(pair_idx=pair_idx, device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=frames)\n",
    "        c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "#             H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "#             rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#             depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "                \n",
    "    imageio.mimwrite(f'{save_dir}/ft_scan{scene}_spiral2.mp4', np.stack(frames), fps=20, quality=10)\n",
    "# plt.imshow(rgb_rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# render path generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_poses = {}\n",
    "datatype = 'val'\n",
    "for i_scene, scene in enumerate(['flower','orchids', 'room','leaves','fern','horns','trex','fortress']):\n",
    "    # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff --imgScale_test {1.0} \\\n",
    "    --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "    c2ws_all = dataset.poses\n",
    "    w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.5, N_views=60) \n",
    "    \n",
    "    render_poses[f'{scene}_near_far_source'] = near_far_source\n",
    "    render_poses[f'{scene}_c2ws_no_ft'] = c2ws_render\n",
    "    render_poses[f'{scene}_intrinsic_no_ft'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n",
    "for i_scene, scene in enumerate(['flower','orchids', 'room','leaves','fern','horns','trex','fortress']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "    # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff --imgScale_test {1.0} \\\n",
    "    --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "    c2ws_all = dataset.poses\n",
    "    w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "    render_poses[f'{scene}_c2ws'] = c2ws_render\n",
    "    render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "#######################################\n",
    "for i_scene, scene in enumerate(['ship','mic','chair','lego','drums','ficus','materials','hotdog']):#\n",
    "\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0}  \\\n",
    "    --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/ckpts//latest.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "\n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "    c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=60)\n",
    "    \n",
    "    render_poses[f'{scene}_c2ws'] = c2ws_render.cpu().numpy()\n",
    "    render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "    \n",
    "##################################################\n",
    "for i_scene, scene in enumerate([1]):\n",
    "\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft --imgScale_test {1.0}   \\\n",
    "    --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/dtu_scan{scene}/ckpts//latest.tar --netwidth 256 --net_type v0 --use_color_volume'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "    imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "    c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=60)\n",
    "    render_poses[f'dtu_c2ws'] = c2ws_render\n",
    "    render_poses[f'dtu_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n",
    "    \n",
    "torch.save(render_poses, './configs/video_path.th')\n",
    "np.save('./configs/video_path.npy',render_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(234)\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "TINY_NUMBER = 1e-6      # float32 only has 7 decimal digits precision\n",
    "\n",
    "def angular_dist_between_2_vectors(vec1, vec2):\n",
    "    vec1_unit = vec1 / (np.linalg.norm(vec1, axis=1, keepdims=True) + TINY_NUMBER)\n",
    "    vec2_unit = vec2 / (np.linalg.norm(vec2, axis=1, keepdims=True) + TINY_NUMBER)\n",
    "    angular_dists = np.arccos(np.clip(np.sum(vec1_unit*vec2_unit, axis=-1), -1.0, 1.0))\n",
    "    return angular_dists\n",
    "\n",
    "\n",
    "def batched_angular_dist_rot_matrix(R1, R2):\n",
    "    '''\n",
    "    calculate the angular distance between two rotation matrices (batched)\n",
    "    :param R1: the first rotation matrix [N, 3, 3]\n",
    "    :param R2: the second rotation matrix [N, 3, 3]\n",
    "    :return: angular distance in radiance [N, ]\n",
    "    '''\n",
    "    assert R1.shape[-1] == 3 and R2.shape[-1] == 3 and R1.shape[-2] == 3 and R2.shape[-2] == 3\n",
    "    return np.arccos(np.clip((np.trace(np.matmul(R2.transpose(0, 2, 1), R1), axis1=1, axis2=2) - 1) / 2.,\n",
    "                             a_min=-1 + TINY_NUMBER, a_max=1 - TINY_NUMBER))\n",
    "\n",
    "\n",
    "def get_nearest_pose_ids(tar_pose, ref_poses, num_select, tar_id=-1, angular_dist_method='vector',\n",
    "                         scene_center=(0, 0, 0)):\n",
    "    '''\n",
    "    Args:\n",
    "        tar_pose: target pose [3, 3]\n",
    "        ref_poses: reference poses [N, 3, 3]\n",
    "        num_select: the number of nearest views to select\n",
    "    Returns: the selected indices\n",
    "    '''\n",
    "    num_cams = len(ref_poses)\n",
    "    # num_select = min(num_select, num_cams-1)\n",
    "    batched_tar_pose = tar_pose[None].repeat(num_cams,axis=0)\n",
    "\n",
    "    if angular_dist_method == 'matrix':\n",
    "        dists = batched_angular_dist_rot_matrix(batched_tar_pose[:, :3, :3], ref_poses[:, :3, :3])\n",
    "    elif angular_dist_method == 'vector':\n",
    "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
    "        ref_cam_locs = ref_poses[:, :3, 3]\n",
    "        scene_center = np.array(scene_center)[None, ...]\n",
    "        tar_vectors = tar_cam_locs - scene_center\n",
    "        ref_vectors = ref_cam_locs - scene_center\n",
    "        dists = angular_dist_between_2_vectors(tar_vectors, ref_vectors)\n",
    "    elif angular_dist_method == 'dist':\n",
    "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
    "        ref_cam_locs = ref_poses[:, :3, 3]\n",
    "        dists = np.linalg.norm(tar_cam_locs - ref_cam_locs, axis=1)\n",
    "    else:\n",
    "        raise Exception('unknown angular distance calculation method!')\n",
    "\n",
    "    if tar_id >= 0:\n",
    "        assert tar_id < num_cams\n",
    "        dists[tar_id] = 1e3  # make sure not to select the target id itself\n",
    "\n",
    "    sorted_ids = np.argsort(dists)\n",
    "    selected_ids = sorted_ids[:num_select]\n",
    "    # print(angular_dists[selected_ids] * 180 / np.pi)\n",
    "    return selected_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "render_poses = {}\n",
    "datatype = 'val'\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "# for i_scene, scene in enumerate(['flower']):#\n",
    "#     # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "#      --dataset_name llff --imgScale_test {1.0} \\\n",
    "#     --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "#     c2ws_all = dataset.poses\n",
    "#     w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "#     pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "#     c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "#     images = []\n",
    "#     for i, c2w in enumerate(c2ws_render):\n",
    "#         nearest_pose_ids = get_nearest_pose_ids(c2w,\n",
    "#                                                 c2ws_all[pair_idx],\n",
    "#                                                 3,\n",
    "#                                                 angular_dist_method='vector')  \n",
    "#         idxs = pair_idx[nearest_pose_ids]\n",
    "        \n",
    "#         im=[]\n",
    "#         List = sorted(glob.glob(f'/mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}/images_4/*'))\n",
    "#         for idx in idxs:\n",
    "#             im.append(cv2.resize(cv2.imread(List[idx]),None,fx=0.25,fy=0.25))\n",
    "#         im = np.concatenate(im,axis=1)\n",
    "#         images.append(im[...,::-1])\n",
    "    \n",
    "#     imageio.mimwrite(f'./results/test4/{scene}.mp4', np.stack(images), fps=20, quality=10)\n",
    "\n",
    "# for i_scene, scene in enumerate(['flower','orchids', 'room','leaves','fern','horns','trex','fortress']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "#     # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "#      --dataset_name llff --imgScale_test {1.0} \\\n",
    "#     --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "#     c2ws_all = dataset.poses\n",
    "#     w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "#     pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "#     c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "#     render_poses[f'{scene}_c2ws'] = c2ws_render\n",
    "#     render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "# #######################################\n",
    "for i_scene, scene in enumerate(['mic']):#\n",
    "\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0}  \\\n",
    "    --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/ckpts//latest.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "\n",
    "    c2ws_all = dataset.load_poses_all()\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "    c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=60).cpu().numpy()\n",
    "    \n",
    "    \n",
    "    images = []\n",
    "    for i, c2w in enumerate(c2ws_render):\n",
    "        nearest_pose_ids = get_nearest_pose_ids(c2w,\n",
    "                                                c2ws_all[pair_idx],\n",
    "                                                3,\n",
    "                                                angular_dist_method='vector')  \n",
    "        idxs = pair_idx[nearest_pose_ids]\n",
    "        im=[]\n",
    "        List = sorted(glob.glob(f'/mnt/new_disk2/anpei/Dataset/nerf_synthetic/{scene}/train/*'))\n",
    "        for idx in idxs:\n",
    "            temp = cv2.imread(f'/mnt/new_disk2/anpei/Dataset/nerf_synthetic/mic/train/r_{idx}.png',-1)\n",
    "            im.append(cv2.resize(temp,None,fx=0.25,fy=0.25))\n",
    "        im = np.concatenate(im,axis=1)\n",
    "        images.append(im[...,[2,1,0,3]])\n",
    "    \n",
    "    imageio.mimwrite(f'./results/test4/{scene}.mp4', np.stack(images), fps=20, quality=10)\n",
    "    \n",
    "# ##################################################\n",
    "# for i_scene, scene in enumerate([1]):\n",
    "\n",
    "#     cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "#      --dataset_name dtu_ft --imgScale_test {1.0}   \\\n",
    "#     --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/dtu_scan{scene}/ckpts//latest.tar --netwidth 256 --net_type v0 --use_color_volume'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "#     imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "#     c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=60)\n",
    "#     render_poses[f'dtu_c2ws'] = c2ws_render\n",
    "#     render_poses[f'dtu_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
